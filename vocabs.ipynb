{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESM-2 vocabulary:\n",
      "Vocabulary size: 33\n",
      "Special tokens:\n",
      "['<cls>', '<pad>', '<eos>', '<unk>', '<null_1>', '<mask>']\n",
      "Standard amino acids:\n",
      "['L', 'A', 'G', 'V', 'S', 'E', 'R', 'T', 'I', 'D', 'P', 'K', 'Q', 'N', 'F', 'Y', 'M', 'H', 'W', 'C', 'X', 'B', 'U', 'Z', 'O']\n"
     ]
    }
   ],
   "source": [
    "# import esm\n",
    "# print('ESM-1b alphabet:')\n",
    "# alphabet = esm.Alphabet.from_architecture('ESM-1b')\n",
    "# print(f'Standard tokens: {alphabet.standard_toks}')\n",
    "# print(f'Prepend tokens: {alphabet.prepend_toks}')\n",
    "# print(f'Append tokens: {alphabet.append_toks}')\n",
    "# print(f'Total vocab size: {len(alphabet)}')\n",
    "\n",
    "from transformers import EsmTokenizer; tokenizer = EsmTokenizer.from_pretrained('facebook/esm2_t30_150M_UR50D')\n",
    "print('ESM-2 vocabulary:')\n",
    "vocab = tokenizer.get_vocab()\n",
    "print(f'Vocabulary size: {len(vocab)}')\n",
    "print('Special tokens:')\n",
    "special_tokens = [k for k in vocab.keys() if k.startswith('<')]\n",
    "print(special_tokens)\n",
    "print('Standard amino acids:')\n",
    "aa_tokens = [k for k in vocab.keys() if k.isalpha() and len(k) == 1]\n",
    "print(aa_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda/envs/byprot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data size: 22401/22507. Discarded: {'bad_chars': 107, 'too_long': 0}.\n",
      "Size. train: 16631, validation: 1516\n",
      "Token mapping:\n",
      "<cls>: 0\n",
      "<pad>: 1\n",
      "<eos>: 2\n",
      "<unk>: 3\n",
      "L: 4\n",
      "A: 5\n",
      "G: 6\n",
      "V: 7\n",
      "S: 8\n",
      "E: 9\n",
      "R: 10\n",
      "T: 11\n",
      "I: 12\n",
      "D: 13\n",
      "P: 14\n",
      "K: 15\n",
      "Q: 16\n",
      "N: 17\n",
      "F: 18\n",
      "Y: 19\n",
      "M: 20\n",
      "H: 21\n",
      "W: 22\n",
      "C: 23\n",
      "X: 24\n",
      "B: 25\n",
      "U: 26\n",
      "Z: 27\n",
      "O: 28\n",
      ".: 29\n",
      "-: 30\n",
      "<null_1>: 31\n",
      "<mask>: 32\n"
     ]
    }
   ],
   "source": [
    "from load_cath import CATHDataModule\n",
    "\n",
    "\n",
    "cath_dm = CATHDataModule(\n",
    "    data_dir=\"data/cath_4.3/\",\n",
    "    chain_set_jsonl=\"chain_set.jsonl\",\n",
    "    chain_set_splits_json=\"chain_set_splits.json\",\n",
    "    alphabet_name='esm2',\n",
    "    max_length=500,\n",
    "    batch_size=32,\n",
    "    max_tokens=6000,\n",
    "    sort=True,\n",
    "    num_workers=0,  # Use 0 for testing\n",
    "    pin_memory=False\n",
    ")\n",
    "cath_dm.setup(stage='fit')\n",
    "alphabet = cath_dm.alphabet\n",
    "\n",
    "print(\"Token mapping:\")\n",
    "for i in range(len(alphabet)):\n",
    "    print(f\"{alphabet.get_tok(i)}: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get attributes of alphabet\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<cls>': 0,\n",
       " '<pad>': 1,\n",
       " '<eos>': 2,\n",
       " '<unk>': 3,\n",
       " 'L': 4,\n",
       " 'A': 5,\n",
       " 'G': 6,\n",
       " 'V': 7,\n",
       " 'S': 8,\n",
       " 'E': 9,\n",
       " 'R': 10,\n",
       " 'T': 11,\n",
       " 'I': 12,\n",
       " 'D': 13,\n",
       " 'P': 14,\n",
       " 'K': 15,\n",
       " 'Q': 16,\n",
       " 'N': 17,\n",
       " 'F': 18,\n",
       " 'Y': 19,\n",
       " 'M': 20,\n",
       " 'H': 21,\n",
       " 'W': 22,\n",
       " 'C': 23,\n",
       " 'X': 24,\n",
       " 'B': 25,\n",
       " 'U': 26,\n",
       " 'Z': 27,\n",
       " 'O': 28,\n",
       " '.': 29,\n",
       " '-': 30,\n",
       " '<null_1>': 31,\n",
       " '<mask>': 32}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EsmTokenizer, EsmModel\n",
    "# Load models\n",
    "tokenizer = EsmTokenizer.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
    "esm_model = EsmModel.from_pretrained(\"facebook/esm2_t30_150M_UR50D\")\n",
    "\n",
    "# Load your ProteinMPNN model\n",
    "# from proteinmpnn import ProteinMPNN\n",
    "# proteinmpnn_model = ProteinMPNN()\n",
    "\n",
    "wrapper = SeqStructEncoderWrapper(esm_model, tokenizer, proteinmpnn_model)\n",
    "\n",
    "# Sample input\n",
    "sequence = \"MKTAYIAKQRQISFVKSHFSRQ\"\n",
    "structure = ...  # your parsed structure input\n",
    "esm_emb, mpnn_emb = wrapper.encode(sequence, structure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byprot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
